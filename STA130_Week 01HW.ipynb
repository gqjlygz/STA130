{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c11dea",
   "metadata": {},
   "source": [
    "\"Pre-lecture\" HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2266d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Display the missing value count\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e88b8",
   "metadata": {},
   "source": [
    "**Summary of Interaction**\n",
    "\n",
    "**Objective:**\n",
    "The goal was to find an interesting dataset with missing values available online through a URL link to a CSV file.\n",
    "\n",
    "**Datasets Provided:**\n",
    "\n",
    "1. **Jeopardy Questions Dataset**\n",
    "   - **Initial Link:** [Jeopardy Questions](https://github.com/ankit21893/Jeopardy-Questions/blob/master/dataset.csv)\n",
    "   - **Issue:** The provided link was a GitHub page link, not a direct link to the raw CSV file, resulting in a 404 error.\n",
    "   - **Correction:** Provided an alternative link which was also found to be broken.\n",
    "\n",
    "2. **Netflix Movies and TV Shows Dataset**\n",
    "   - **Initial Link:** [Netflix Movies and TV Shows](https://raw.githubusercontent.com/shivamb/netflix-shows/master/netflix_titles.csv)\n",
    "   - **Issue:** This link also proved to be inaccessible, resulting in a 404 error.\n",
    "\n",
    "3. **Alternative Datasets:**\n",
    "   - **Iris Dataset:** [Iris Dataset](https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv)\n",
    "     - **Issue:** Verified that this dataset does not have missing values.\n",
    "   - **Titanic Dataset:** [Titanic Dataset](https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv)\n",
    "     - **Has Missing Values:** Multiple columns including `Age`, `Cabin`, and `Embarked` contain missing values.\n",
    "   - **Wine Quality Dataset:** [Wine Quality Dataset](https://raw.githubusercontent.com/selva86/datasets/master/winequality-red.csv)\n",
    "     - **Issue:** Verified that this dataset does not have missing values.\n",
    "   - **COVID-19 Dataset:** [COVID-19 Dataset](https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv)\n",
    "     - **Issue:** Verified that this dataset is typically complete with no missing values.\n",
    "   - **House Prices Dataset:** [House Prices Dataset](https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv)\n",
    "     - **Has Missing Values:** Contains some missing values in various columns.\n",
    "   - **Air Quality Dataset:** [Air Quality Dataset](https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv)\n",
    "     - **Has Missing Values:** Includes missing values in the time series data.\n",
    "\n",
    "**Conclusion:**\n",
    "After reviewing several datasets, the **Titanic Dataset** and the **House Prices Dataset** were confirmed to contain missing values and are suitable for analysis involving missing data. These datasets are available online and meet the requirement of having missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef740b89",
   "metadata": {},
   "source": [
    "Chat history with Chatgpt:\n",
    "https://chatgpt.com/share/c70cca85-93ad-467e-8781-24a3bd9b9d26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65760173",
   "metadata": {},
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "\"Post-lecture\" HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450b31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display summary statistics\n",
    "summary_statistics = data.describe()\n",
    "print(summary_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307686b",
   "metadata": {},
   "source": [
    "7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b064a",
   "metadata": {},
   "source": [
    "\"df.dropna()\" is suitable when there are few missing values and can retain more non-missing data\n",
    "\n",
    "Example: Suppose there is a medical dataset with the following columns: PatientID, Age, Gender, BloodPressure, and HeartRate. There are 1000 rows, and only 10 rows have missing values in either the BloodPressure or HeartRate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f97818",
   "metadata": {},
   "source": [
    "7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefab44",
   "metadata": {},
   "source": [
    "\"del df['col']\" is suitable when columns have a high proportion of missing data.\n",
    "\n",
    "Example: Suppose you have a dataset with columns: CustomerID, Age, Gender, Income, and FavoriteColor. The FavoriteColor column has 80% missing values, while other columns have very few missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27683fe4",
   "metadata": {},
   "source": [
    "7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c842a9",
   "metadata": {},
   "source": [
    "When both del df['col'] and df.dropna() are used together, by removing columns with a high proportion of missing values first, you avoid unnecessarily deleting valuable rows with df.dropna()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4995123",
   "metadata": {},
   "source": [
    "7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e928bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before dropping columns:\n",
      " PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "Columns after dropping 'Age' and 'Cabin':\n",
      " Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Embarked'],\n",
      "      dtype='object')\n",
      "\n",
      "Missing values after dropping columns:\n",
      " PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Embarked       2\n",
      "dtype: int64\n",
      "\n",
      "Shape of the dataset after removing rows with missing data: (889, 10)\n",
      "\n",
      "Missing values after removing rows with missing data:\n",
      " PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Display the missing value count\n",
    "print(\"Missing values before dropping columns:\\n\", missing_values)\n",
    "\n",
    "# Drop 'Age' and 'Cabin' columns using del\n",
    "del data['Age']\n",
    "del data['Cabin']\n",
    "\n",
    "# Verify the columns have been removed\n",
    "print(\"\\nColumns after dropping 'Age' and 'Cabin':\\n\", data.columns)\n",
    "\n",
    "# Display the missing value count after dropping columns\n",
    "missing_values_after = data.isnull().sum()\n",
    "print(\"\\nMissing values after dropping columns:\\n\", missing_values_after)\n",
    "\n",
    "# Remove rows with any remaining missing data\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "# Display the shape of the dataset after dropping rows with missing values\n",
    "print(\"\\nShape of the dataset after removing rows with missing data:\", data_cleaned.shape)\n",
    "\n",
    "# Display missing values count after dropping rows\n",
    "missing_values_cleaned = data_cleaned.isnull().sum()\n",
    "print(\"\\nMissing values after removing rows with missing data:\\n\", missing_values_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03f337",
   "metadata": {},
   "source": [
    "The reason for using `del df['col']` to remove the `Age` and `Cabin` columns is that:\n",
    "The `Cabin` and `Age`column have more than 50% missing values, making it less useful for analysis and likely to skew results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec07ee",
   "metadata": {},
   "source": [
    "After removing the columns which have a large propotion of missing data, I use data.dropna() to remove the remaining rows with missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe441e",
   "metadata": {},
   "source": [
    "#\n",
    "8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6866bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std  min       25%      50%   75%       max\n",
      "Pclass                                                                     \n",
      "1       216.0  84.154687  78.380373  0.0  30.92395  60.2875  93.5  512.3292\n",
      "2       184.0  20.662183  13.417399  0.0  13.00000  14.2500  26.0   73.5000\n",
      "3       491.0  13.675550  11.778142  0.0   7.75000   8.0500  15.5   69.5500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Group by 'Pclass' and describe 'Fare'\n",
    "grouped_stats = data.groupby(\"Pclass\")[\"Fare\"].describe()\n",
    "\n",
    "# Display the grouped statistics\n",
    "print(grouped_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bd0d3",
   "metadata": {},
   "source": [
    "8.2\n",
    "\n",
    "df.describe():  Analyzes the entire dataset and provides the total count of non-missing values for each column. This shows the amount of valid data for each column across the whole dataset.\n",
    "\n",
    "df.groupby(\"col1\")[\"col2\"].describe():  Computes statistics within each group defined by col1. Therefore, the count values reflect the number of non-missing values for col2 within each group. This shows the data completeness within different subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb069e",
   "metadata": {},
   "source": [
    "8.3\n",
    "\n",
    "ChatGPT search for the error provides the necessary toubleshooting help more quickly than google "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367795f8",
   "metadata": {},
   "source": [
    "#\n",
    "9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba46e5",
   "metadata": {},
   "source": [
    "YES, I reviewed the course wiki-textbook and interacted with a ChatBot to help me understand all the material in the tutorial and lecture that I didn't quite follow when I first saw it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
